import pandas as pd
import numpy as np
import os
import glob
from scipy.stats import linregress
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import LeaveOneOut
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
import warnings
from pathlib import Path
import sys

# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œå¿½ç•¥è­¦å‘Š
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
warnings.filterwarnings('ignore')

class BatteryLifePredictionModel:
    def __init__(self, data_folder=None, num_early_cycles=40):
        """
        åˆå§‹åŒ–ç”µæ± å¯¿å‘½é¢„æµ‹æ¨¡å‹
        
        Args:
            data_folder: æ•°æ®æ–‡ä»¶å¤¹è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨æŸ¥æ‰¾
            num_early_cycles: ä½¿ç”¨çš„æ—©æœŸå¾ªç¯æ•°æ®é‡
        """
        self.num_early_cycles = num_early_cycles
        self.data_folder = self._find_data_folder(data_folder)
        self.feature_scaler = MinMaxScaler()  # æ”¹ä¸ºMinMaxScaler
        self.target_scaler = StandardScaler()
        self.model = None
        self.results = {}
        
    def _find_data_folder(self, data_folder):
        """è‡ªåŠ¨æŸ¥æ‰¾æ•°æ®æ–‡ä»¶å¤¹"""
        if data_folder and os.path.exists(data_folder):
            return data_folder
            
        # å¯èƒ½çš„æ•°æ®è·¯å¾„
        possible_paths = [
            "normalized_features"
        ]
        
        for path in possible_paths:
            if os.path.exists(path):
                csv_files = glob.glob(os.path.join(path, "*.csv"))
                if csv_files:
                    print(f"æ‰¾åˆ°æ•°æ®æ–‡ä»¶å¤¹: {path}")
                    return path
                    
        raise FileNotFoundError("æœªæ‰¾åˆ°åŒ…å«CSVæ–‡ä»¶çš„æ•°æ®æ–‡ä»¶å¤¹ï¼Œè¯·æ‰‹åŠ¨æŒ‡å®šdata_folderå‚æ•°")

    def load_csv_data(self):
        """åŠ è½½CSVæ•°æ®"""
        csv_files = glob.glob(os.path.join(self.data_folder, "*.csv"))
        csv_files = sorted(csv_files)
        
        if not csv_files:
            raise FileNotFoundError(f"åœ¨ {self.data_folder} ä¸­æœªæ‰¾åˆ°CSVæ–‡ä»¶")
            
        print(f"æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶")
        
        early_cycle_data_list = []
        final_cycle_counts = []
        file_names = []
        
        for file_path in csv_files:
            try:
                # å°è¯•ä¸åŒçš„ç¼–ç æ–¹å¼è¯»å–CSV
                df_full = None
                for encoding in ['utf-8', 'gbk', 'gb2312', 'latin-1']:
                    try:
                        df_full = pd.read_csv(file_path, encoding=encoding)
                        break
                    except UnicodeDecodeError:
                        continue
                
                if df_full is None:
                    print(f"è­¦å‘Š: æ— æ³•è¯»å–æ–‡ä»¶ {file_path}")
                    continue
                
                # æ£€æŸ¥æ•°æ®æ ¼å¼
                if df_full.shape[1] < 15:
                    print(f"è­¦å‘Š: æ–‡ä»¶ {file_path} åˆ—æ•°ä¸è¶³15åˆ—ï¼Œè·³è¿‡")
                    continue
                
                total_cycles = len(df_full)
                num_cycles_to_read = min(self.num_early_cycles, total_cycles)
                
                # æå–å‰15åˆ—ä½œä¸ºç‰¹å¾
                early_features = df_full.iloc[:num_cycles_to_read, :15].values
                
                # æ£€æŸ¥æ•°æ®ä¸­æ˜¯å¦æœ‰æ— æ•ˆå€¼
                if np.any(np.isnan(early_features)) or np.any(np.isinf(early_features)):
                    # å¡«å……æ— æ•ˆå€¼
                    early_features = np.nan_to_num(early_features, nan=0.0, posinf=1.0, neginf=-1.0)
                
                cycle_numbers = np.arange(1, num_cycles_to_read + 1).reshape(-1, 1)
                early_data_with_cycle = np.hstack((early_features, cycle_numbers))
                
                early_cycle_data_list.append(early_data_with_cycle)
                final_cycle_counts.append(total_cycles)
                file_names.append(os.path.basename(file_path))
                
                print(f"å¤„ç†å®Œæˆ: {os.path.basename(file_path)} - æ—©æœŸæ•°æ®å½¢çŠ¶: {early_data_with_cycle.shape}, æœ€ç»ˆå¾ªç¯æ•°: {total_cycles}")
                
            except Exception as e:
                print(f"å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {str(e)}")
                continue
        
        if not early_cycle_data_list:
            raise ValueError("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ•°æ®æ–‡ä»¶")
            
        final_cycle_counts = np.array(final_cycle_counts)
        
        print(f"\næ•°æ®åŠ è½½å®Œæˆ!")
        print(f"æˆåŠŸåŠ è½½ {len(early_cycle_data_list)} ä¸ªç”µæ± æ ·æœ¬çš„æ—©æœŸå¾ªç¯æ•°æ®")
        print(f"æœ€ç»ˆå¾ªç¯æ•°æ•°ç»„å½¢çŠ¶: {final_cycle_counts.shape}")
        
        return early_cycle_data_list, final_cycle_counts, file_names

    def extract_battery_level_features(self, early_cycle_data_list, file_names):
        """æå–ç”µæ± çº§åˆ«ç‰¹å¾"""
        X_engineered = []
        print(f"\nå¼€å§‹ç‰¹å¾å·¥ç¨‹...")
        
        for i, battery_data in enumerate(early_cycle_data_list):
            engineered_features_for_this_battery = []
            
            for feature_idx in range(battery_data.shape[1]):
                feature_series = battery_data[:, feature_idx]
                
                if len(feature_series) > 0:
                    # åŸºæœ¬ç»Ÿè®¡ç‰¹å¾
                    engineered_features_for_this_battery.append(np.mean(feature_series))
                    engineered_features_for_this_battery.append(np.std(feature_series) if len(feature_series) > 1 else 0.0)
                    engineered_features_for_this_battery.append(np.min(feature_series))
                    engineered_features_for_this_battery.append(np.max(feature_series))
                    engineered_features_for_this_battery.append(feature_series[0])
                    engineered_features_for_this_battery.append(feature_series[-1])
                    
                    # è¶‹åŠ¿ç‰¹å¾
                    if len(feature_series) > 1:
                        if feature_idx == 15:  # Cycle number feature
                            x_values = feature_series
                        else:
                            x_values = np.arange(1, len(feature_series) + 1)
                        
                        try:
                            slope, _, _, _, _ = linregress(x_values, feature_series)
                            if np.isnan(slope) or np.isinf(slope):
                                slope = 0.0
                        except:
                            slope = 0.0
                        
                        engineered_features_for_this_battery.append(slope)
                    else:
                        engineered_features_for_this_battery.append(0.0)
                else:
                    engineered_features_for_this_battery.extend([0.0] * 7)
            
            X_engineered.append(engineered_features_for_this_battery)
            
            if (i + 1) % 10 == 0:
                print(f"  å·²å¤„ç† {i + 1}/{len(early_cycle_data_list)} ä¸ªç”µæ± æ ·æœ¬")
        
        X_engineered_array = np.array(X_engineered)
        
        # æ£€æŸ¥ç‰¹å¾æ•°ç»„ä¸­çš„æ— æ•ˆå€¼
        if np.any(np.isnan(X_engineered_array)) or np.any(np.isinf(X_engineered_array)):
            X_engineered_array = np.nan_to_num(X_engineered_array, nan=0.0, posinf=1.0, neginf=-1.0)
            print("è­¦å‘Š: æ£€æµ‹åˆ°å¹¶ä¿®å¤äº†ç‰¹å¾æ•°ç»„ä¸­çš„æ— æ•ˆå€¼")
        
        print(f"ç‰¹å¾å·¥ç¨‹å®Œæˆ! æœ€ç»ˆå·¥ç¨‹ç‰¹å¾å½¢çŠ¶: {X_engineered_array.shape}")
        return X_engineered_array

    def train_and_evaluate_model_loocv(self, X_engineered, final_cycle_counts):
        """ä½¿ç”¨ç•™ä¸€æ³•äº¤å‰éªŒè¯è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹"""
        print(f"\nå¼€å§‹æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼° (LOOCV)...")
        print(f"è¾“å…¥ç‰¹å¾å½¢çŠ¶: {X_engineered.shape}, ç›®æ ‡å˜é‡å½¢çŠ¶: {final_cycle_counts.shape}")

        # ç‰¹å¾MinMaxå½’ä¸€åŒ–
        X_scaled = self.feature_scaler.fit_transform(X_engineered)
        print(f"ç‰¹å¾MinMaxå½’ä¸€åŒ–å®Œæˆ. å½’ä¸€åŒ–åXçš„èŒƒå›´: [{np.min(X_scaled):.4f}, {np.max(X_scaled):.4f}]")

        # ç›®æ ‡å˜é‡log1på˜æ¢åæ ‡å‡†åŒ–
        y_log1p = np.log1p(final_cycle_counts)  # log1på˜æ¢
        print(f"ç›®æ ‡å˜é‡log1på˜æ¢å®Œæˆ. å˜æ¢åYçš„èŒƒå›´: [{np.min(y_log1p):.4f}, {np.max(y_log1p):.4f}]")
        
        y_scaled = self.target_scaler.fit_transform(y_log1p.reshape(-1, 1)).flatten()
        print(f"ç›®æ ‡å˜é‡æ ‡å‡†åŒ–å®Œæˆ. æ ‡å‡†åŒ–åYçš„å‡å€¼å’Œæ ‡å‡†å·®: {np.mean(y_scaled):.2f}, {np.std(y_scaled):.2f}")
        
        # åˆ›å»ºéšæœºæ£®æ—æ¨¡å‹
        self.model = RandomForestRegressor(
            n_estimators=500,
            max_depth=20,
            min_samples_split=2,
            min_samples_leaf=1,
            max_features=1.0,
            warm_start=True,
            random_state=1247,
            n_jobs=-1,
            min_weight_fraction_leaf=0.0,
        )

        # ç•™ä¸€æ³•äº¤å‰éªŒè¯
        loo = LeaveOneOut()
        y_true_all = []
        y_pred_all = []
        
        total_folds = len(X_scaled)
        for i, (train_index, test_index) in enumerate(loo.split(X_scaled)):
            X_train, X_test = X_scaled[train_index], X_scaled[test_index]
            y_train, y_test = y_scaled[train_index], y_scaled[test_index]

            self.model.fit(X_train, y_train)
            y_pred_fold_scaled = self.model.predict(X_test)
            
            y_true_all.extend(y_test)
            y_pred_all.extend(y_pred_fold_scaled)
            
            if (i + 1) % 10 == 0:
                print(f"  å·²å®Œæˆ {i + 1}/{total_folds} æŠ˜äº¤å‰éªŒè¯")
    
        # è½¬æ¢å›åŸå§‹å°ºåº¦
        # å…ˆåæ ‡å‡†åŒ–ï¼Œå†ålog1på˜æ¢
        y_true_log1p = self.target_scaler.inverse_transform(np.array(y_true_all).reshape(-1, 1)).flatten()
        y_pred_log1p = self.target_scaler.inverse_transform(np.array(y_pred_all).reshape(-1, 1)).flatten()
        
        y_true_original = np.expm1(y_true_log1p)  # ålog1på˜æ¢
        y_pred_original = np.expm1(y_pred_log1p)  # ålog1på˜æ¢
        
        # ç¡®ä¿é¢„æµ‹å€¼ä¸ä¸ºè´Ÿæ•°
        y_pred_original = np.maximum(y_pred_original, 0)

        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        overall_rmse = np.sqrt(mean_squared_error(y_true_original, y_pred_original))
        overall_mae = mean_absolute_error(y_true_original, y_pred_original)
        overall_r2 = r2_score(y_true_original, y_pred_original)

        print(f"\n{'='*60}")
        print(f"LOOCV éšæœºæ£®æ—æ¨¡å‹è¯„ä¼°ç»“æœ (åŸå§‹å°ºåº¦ï¼Œä½¿ç”¨log1på˜æ¢ï¼Œç‰¹å¾MinMaxå½’ä¸€åŒ–)")
        print(f"{'='*60}")
        print(f"  RMSE: {overall_rmse:.4f}")
        print(f"  MAE:  {overall_mae:.4f}")
        print(f"  RÂ² Score: {overall_r2:.4f}")
        
        print(f"\né¢„æµ‹ç¤ºä¾‹ (å‰10ä¸ªæ ·æœ¬):")
        print(f"{'å®é™…å€¼':<10} {'é¢„æµ‹å€¼':<10} {'è¯¯å·®':<10} {'è¯¯å·®ç‡%':<10}")
        print("-" * 50)
        for i in range(min(10, len(y_true_original))):
            actual = y_true_original[i]
            predicted = y_pred_original[i]
            error = abs(actual - predicted)
            error_rate = (error / actual * 100) if actual != 0 else np.inf
            print(f"{actual:<10.2f} {predicted:<10.2f} {error:<10.2f} {error_rate:<10.2f}%")
        
        # é‡æ–°è®­ç»ƒæ¨¡å‹ç”¨äºåç»­é¢„æµ‹
        self.model.fit(X_scaled, y_scaled)
        
        self.results = {
            'model': self.model,
            'feature_scaler': self.feature_scaler,  # ä¿å­˜ç‰¹å¾å½’ä¸€åŒ–å™¨
            'target_scaler': self.target_scaler,
            'overall_rmse': overall_rmse,
            'overall_mae': overall_mae,
            'overall_r2': overall_r2,
            'y_true_original': y_true_original,
            'y_pred_original': y_pred_original,
            'X_scaled': X_scaled,  # ä¿å­˜å½’ä¸€åŒ–åçš„ç‰¹å¾
            'feature_importances': self.model.feature_importances_,
            'use_log1p': True  # æ ‡è®°ä½¿ç”¨äº†log1på˜æ¢
        }
        
        return self.results

    def plot_prediction_results(self):
        """ç»˜åˆ¶é¢„æµ‹ç»“æœ"""
        if not self.results:
            print("è¯·å…ˆè®­ç»ƒæ¨¡å‹")
            return
            
        y_true = self.results['y_true_original']
        y_pred = self.results['y_pred_original']
        r2 = self.results['overall_r2']
        
        fig, axes = plt.subplots(1, 2, figsize=(14, 6))
        
        # å®é™…å€¼ vs é¢„æµ‹å€¼
        axes[0].scatter(y_true, y_pred, alpha=0.7, color='blue')
        max_val = max(y_true.max(), y_pred.max())
        min_val = min(y_true.min(), y_pred.min())
        axes[0].plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)
        axes[0].set_xlabel('å®é™…å¾ªç¯æ•°')
        axes[0].set_ylabel('é¢„æµ‹å¾ªç¯æ•°')
        axes[0].set_title(f'LOOCV å®é™…å€¼ vs é¢„æµ‹å€¼ (RÂ² = {r2:.4f})')
        axes[0].grid(True, alpha=0.3)
        axes[0].set_aspect('equal', adjustable='box')

        # æ®‹å·®å›¾
        residuals = y_true - y_pred
        axes[1].scatter(y_pred, residuals, alpha=0.7, color='green')
        axes[1].axhline(y=0, color='r', linestyle='--')
        axes[1].set_xlabel('é¢„æµ‹å¾ªç¯æ•°')
        axes[1].set_ylabel('æ®‹å·® (å®é™…å€¼ - é¢„æµ‹å€¼)')
        axes[1].set_title('LOOCV æ®‹å·®å›¾')
        axes[1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        try:
            plt.savefig('prediction_results.png', dpi=300, bbox_inches='tight')
            print("é¢„æµ‹ç»“æœå›¾å·²ä¿å­˜ä¸º prediction_results.png")
        except:
            print("ä¿å­˜å›¾ç‰‡å¤±è´¥ï¼Œä½†æ˜¾ç¤ºæ­£å¸¸")
        
        plt.show()

    def visualize_feature_importance(self, num_original_features=16, n_top=20):
        """å¯è§†åŒ–ç‰¹å¾é‡è¦æ€§"""
        if not self.results:
            print("è¯·å…ˆè®­ç»ƒæ¨¡å‹")
            return
            
        feature_importances = self.results['feature_importances']
        statistic_names = ['Mean', 'Std', 'Min', 'Max', 'First', 'Last', 'Slope']
        
        # ç”Ÿæˆç‰¹å¾åç§°
        engineered_feature_names = []
        for i in range(num_original_features):
            original_feature_name = f'Original_F{i}' if i < 15 else 'Cycle_Number'
            for stat_name in statistic_names:
                engineered_feature_names.append(f'{original_feature_name}_{stat_name}')

        feature_df = pd.DataFrame({
            'feature_name': engineered_feature_names,
            'importance': feature_importances
        }).sort_values('importance', ascending=False)
        
        top_features = feature_df.head(n_top)
        
        plt.figure(figsize=(12, 8))
        sns.barplot(x='importance', y='feature_name', data=top_features, palette='viridis')
        plt.xlabel('ç‰¹å¾é‡è¦æ€§')
        plt.ylabel('å·¥ç¨‹ç‰¹å¾åç§°')
        plt.title(f'å‰ {n_top} ä¸ªå·¥ç¨‹ç‰¹å¾é‡è¦æ€§')
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        try:
            plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')
            print("ç‰¹å¾é‡è¦æ€§å›¾å·²ä¿å­˜ä¸º feature_importance.png")
        except:
            print("ä¿å­˜å›¾ç‰‡å¤±è´¥ï¼Œä½†æ˜¾ç¤ºæ­£å¸¸")
        
        plt.show()

    def save_results_to_file(self, file_names, output_file='model_evaluation_results.txt'):
        """ä¿å­˜è¯„ä¼°ç»“æœåˆ°æ–‡ä»¶"""
        if not self.results:
            print("è¯·å…ˆè®­ç»ƒæ¨¡å‹")
            return
            
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("éšæœºæ£®æ—æ¨¡å‹è¯„ä¼°æŠ¥å‘Š (ç‰¹å¾MinMaxå½’ä¸€åŒ–)\n")
            f.write("=" * 50 + "\n\n")
            
            f.write(f"æ•°æ®é›†ä¿¡æ¯:\n")
            f.write(f"  ç”µæ± æ ·æœ¬æ•°é‡: {len(self.results['y_true_original'])}\n")
            f.write(f"  å·¥ç¨‹ç‰¹å¾æ•°é‡: {self.results['X_scaled'].shape[1]}\n\n")
            
            f.write(f"æ¨¡å‹æ€§èƒ½æŒ‡æ ‡ (LOOCV, åŸå§‹å°ºåº¦):\n")
            f.write(f"  RMSE: {self.results['overall_rmse']:.4f}\n")
            f.write(f"  MAE:  {self.results['overall_mae']:.4f}\n")
            f.write(f"  RÂ²:   {self.results['overall_r2']:.4f}\n\n")
            
            f.write(f"è¯¦ç»†é¢„æµ‹ç»“æœ:\n")
            f.write(f"{'æ–‡ä»¶å':<30} {'å®é™…å€¼':<10} {'é¢„æµ‹å€¼':<10} {'ç»å¯¹è¯¯å·®':<10} {'è¯¯å·®ç‡%':<10}\n")
            f.write("-" * 75 + "\n")
            
            for i in range(len(self.results['y_true_original'])):
                actual = self.results['y_true_original'][i]
                predicted = self.results['y_pred_original'][i]
                error = abs(actual - predicted)
                error_rate = (error / actual * 100) if actual != 0 else np.inf
                f.write(f"{file_names[i]:<30} {actual:<10.2f} {predicted:<10.2f} {error:<10.2f} {error_rate:<10.2f}\n")

    def save_model(self, model_dir='models'):
        """ä¿å­˜æ¨¡å‹å’Œé¢„å¤„ç†å™¨"""
        if not self.results:
            print("è¯·å…ˆè®­ç»ƒæ¨¡å‹")
            return
            
        # åˆ›å»ºæ¨¡å‹ç›®å½•
        Path(model_dir).mkdir(exist_ok=True)
        
        # ä¿å­˜æ¨¡å‹å’Œé¢„å¤„ç†å™¨
        with open(f'{model_dir}/final_rf_model.pkl', 'wb') as f:
            pickle.dump(self.model, f)
        with open(f'{model_dir}/feature_scaler.pkl', 'wb') as f:
            pickle.dump(self.feature_scaler, f)
        with open(f'{model_dir}/target_scaler.pkl', 'wb') as f:
            pickle.dump(self.target_scaler, f)
    
        print(f"\næ¨¡å‹å’Œé¢„å¤„ç†å™¨å·²ä¿å­˜åˆ° {model_dir}/ ç›®å½•")
        print("ä¿å­˜çš„æ–‡ä»¶åŒ…æ‹¬:")
        print(f"- {model_dir}/final_rf_model.pkl: è®­ç»ƒå¥½çš„éšæœºæ£®æ—æ¨¡å‹")
        print(f"- {model_dir}/feature_scaler.pkl: ç‰¹å¾MinMaxå½’ä¸€åŒ–å™¨")
        print(f"- {model_dir}/target_scaler.pkl: ç›®æ ‡å˜é‡æ ‡å‡†åŒ–å™¨")

    def run_complete_pipeline(self):
        """è¿è¡Œå®Œæ•´çš„é¢„æµ‹æµæ°´çº¿"""
        try:
            # 1. åŠ è½½æ•°æ®
            early_data_list, final_cycle_counts, file_names = self.load_csv_data()
            
            # 2. ç‰¹å¾å·¥ç¨‹
            X_engineered = self.extract_battery_level_features(early_data_list, file_names)
            
            # 3. è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹
            self.train_and_evaluate_model_loocv(X_engineered, final_cycle_counts)
            
            # 4. å¯è§†åŒ–ç»“æœ
            self.plot_prediction_results()
            self.visualize_feature_importance()
            
            # 5. ä¿å­˜ç»“æœ
            self.save_results_to_file(file_names)
            self.save_model()
            
            print("\nâœ… å®Œæ•´çš„é¢„æµ‹æµæ°´çº¿æ‰§è¡Œå®Œæˆ!")
            
        except Exception as e:
            print(f"âŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
            import traceback
            traceback.print_exc()

    def predict(self, X_new):
        """å¯¹æ–°æ•°æ®è¿›è¡Œé¢„æµ‹"""
        if self.model is None:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨è®­ç»ƒæ–¹æ³•")
        
        # ç‰¹å¾MinMaxå½’ä¸€åŒ–
        X_new_scaled = self.feature_scaler.transform(X_new)
    
        # æ¨¡å‹é¢„æµ‹ (å¾—åˆ°æ ‡å‡†åŒ–çš„log1på€¼)
        y_pred_scaled = self.model.predict(X_new_scaled)
        
        # åæ ‡å‡†åŒ–å¾—åˆ°log1på€¼
        y_pred_log1p = self.target_scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()
        
        # ålog1på˜æ¢å¾—åˆ°åŸå§‹å°ºåº¦çš„é¢„æµ‹å€¼
        y_pred_original = np.expm1(y_pred_log1p)
        
        # ç¡®ä¿é¢„æµ‹å€¼ä¸ä¸ºè´Ÿæ•°
        y_pred_original = np.maximum(y_pred_original, 0)
        
        return y_pred_original

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”‹ ç”µæ± å¯¿å‘½é¢„æµ‹æ¨¡å‹")
    print("=" * 50)
    
    # åˆ›å»ºæ¨¡å‹å®ä¾‹
    model = BatteryLifePredictionModel(
        data_folder=None,  # è‡ªåŠ¨æŸ¥æ‰¾æ•°æ®æ–‡ä»¶å¤¹
        num_early_cycles=40
    )
    
    # è¿è¡Œå®Œæ•´æµæ°´çº¿
    model.run_complete_pipeline()

if __name__ == "__main__":
    main()